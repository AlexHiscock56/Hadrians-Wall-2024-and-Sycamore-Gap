#LDA for Twitter

if __name__ == "__main__":

    ### Topic model ###
    #This code is adapted from Bonacchi and Kryzanska (2021). The original script is found here: https://github.com/IARHeritages/HeritageTribalism_BigData/blob/main/codes/mongo/05_02_Topic_assignment.js . I also used this guide https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21 
    
    ## This code creates the LDA topic model for the unique documents in the Twitter dataset, for topic numbers n = 2:31,
    ## It is designed to handle a json file of Text data, that also has a Code column. There is one type of code for this template: type a) TT.1
    ## it calculates the coherence scores for the topic and creates and intertopic-distance visualisation for the model with the highest coherence score, with the LDAvis.
    ## it then also assignes the dominant topic and topic scores to all Text documents utilised, and intergrates these with a master file in a Mongodb database

    ##Make sure these are installed 
    #pip install pymongo
    #pip install Scipy
    #pip install numpy
    #pip install nltk
    #pip install pyLDAvis
    #pip install gensim

    #Import relevant libraries:
    import pymongo
    import time
    import gensim
    import os
    import csv
    import re
    import operator
    import warnings
    import numpy as np
    import json
    from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel
    from gensim.corpora import Dictionary
    from gensim.parsing.preprocessing import strip_punctuation
    from pprint import pprint
    from gensim.corpora.mmcorpus import MmCorpus
    from gensim.models import ldamulticore
    from gensim import models
    from nltk.corpus import stopwords
    import pyLDAvis.gensim
    import gc
    import logging
    from nltk.corpus import stopwords
    import nltk
    nltk.download('stopwords')
    nltk.download('wordnet')
    from nltk.tokenize import word_tokenize
    nltk.download('punkt')
    from nltk.stem.wordnet import WordNetLemmatizer
    from gensim.models import Phrases
    from gensim.corpora import MmCorpus
    from gensim.test.utils import get_tmpfile 
    from collections import OrderedDict
    from pymongo import MongoClient
    from bson import ObjectId

    # Set up logging to console to monitor the progress of the topic model:
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    # set a format which is simpler for console use
    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger('').addHandler(console)
    logger = logging.getLogger(__name__)

    ### Set up workspace

    # Set working directory
    os.chdir('/chss.datastore.ed.ac.uk/chss/hca/users/s2442997/PhD/Code/Hadrians Wall')

    # Connect to the MongoDB client
    client = MongoClient('mongodb+srv://alexbrianhiscock:AntonineWall142@cluster0.irkomef.mongodb.net/')

    # Specify the database and collection
    db = client['Hadrians_Wall']
    Textcollection = db['NoRTsTexts_Day_2_Tweets']

    # Load the documents from the collection into a Python list
    Twitter_texts = list(Textcollection.find())
    
       # Function to convert ObjectId to str and remove if required
    def convert_id(entry):
        if isinstance(entry, dict):
            for key, value in list(entry.items()):
                if isinstance(value, ObjectId):
                    # Convert ObjectId to str
                    entry[key] = str(value)
                elif isinstance(value, dict) or isinstance(value, list):
                    # Recursively call convert_id to handle nested dictionaries and lists
                    convert_id(value)
        elif isinstance(entry, list):
            for item in entry:
                # Recursively call convert_id to handle nested dictionaries and lists
                convert_id(item)

    # Assuming Twitter_texts is your data loaded from MongoDB
    convert_id(Twitter_texts) 
    
    # Save the data to a JSON file
    with open('Twitter_Texts.json', 'w') as file:
         json.dump(Twitter_texts, file, indent=4)  
    
    # Define the paths to input:
    file_path = 'Twitter_Texts.json'

    # Define the paths to outputs
    path2corpus= "outputs/topic_models/corpus.mm"
    path2dictionary= "outputs/topic_models/dictionary.mm"
    path2model= "outputs/topic_models/models_.mm"
    path2coherence = "outputs/03_01_01_coherenceScores.csv" # Path to model coherence scores
    path2html = "outputs/03_01_02_topic_model.html" # Path to the best model visualisation in html

    # Define language to use for the model, and the threshold for the number of topics
    language='english'
    max_topics=31

    # Load the JSON file containing all texts
    with open(file_path, 'r', encoding='utf8') as file:
        data = json.load(file)

    # Extracting texts and ignoring the codes and dates
    texts = [item['Text'] for item in data if 'Text' in item]
    #Getting codes as well
    codes = [item['Code'] for item in data if 'Code' in item]

    # Removing links and usernames if they appear in the text/Might edit to keep links to establish linking behaviour within topics:
    i=0
    j=len(texts)
    while(i<j):
        texts[i] = re.sub(r'http\S+', '', texts[i])
        texts[i] = re.sub(r'@\S+', '', texts[i])
        texts[i] = re.sub(r'[^\w\s]', '', texts[i])  # Remove punctuation
        i=i+1    

    # Tokenize the text into words
    texts = [word_tokenize(text.lower()) for text in texts]  # Converts to lowercase and tokenizes

    # Import and define stpowords:
    nltk.download('stopwords')
    stops = set(stopwords.words('english'))
    #Also add new stopwords. The search terms are added as most primary content will contain them.
    new_stops = set(["hadrian","wall", "mention"])

    ## Get rid of english stopwords and user defined stopwords:
    texts = [[word for word in text if word not in stops] for text in texts]
    texts = [[word for word in text if word not in new_stops] for text in texts]

    #Lemmatize all the words in the document:
    nltk.download('wordnet')
    lemmatizer = WordNetLemmatizer()
    texts= [[lemmatizer.lemmatize(token) for token in text] for text in texts]

    # Create bigrams and trigrams:

    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).
    bigram = Phrases(texts, min_count=20)
    for idx in range(len(texts)):
        for token in bigram[texts[idx]]:
            if '_' in token:
                # Token is a bigram, add to document.
                texts[idx].append(token)

    # Make dictionary and the corpus
    train_texts = texts 
    dictionary = Dictionary(train_texts)
    corpus = [dictionary.doc2bow(text) for text in train_texts]

    ### Save corpus and dictionary:  
    MmCorpus.serialize(path2corpus, corpus)
    mm = MmCorpus(path2corpus)
    dictionary.save_as_text(path2dictionary)
    dictionary = Dictionary.load_from_text(path2dictionary)

    # Set up the list to hold coherence values for each topic:
    c_v = []
    # Loop over to create models with 2 to 30 topics, and calculate coherence scores for it:
    for num_topics in range(2, max_topics):
        print(num_topics)
        lm = models.LdaMulticore(corpus=mm, num_topics=num_topics,     id2word=dictionary,chunksize=9000,passes=100,eval_every=1,iterations=500,workers=4) # Create a model for num_topics topics
        print("Calculating coherence score...")
        cm = CoherenceModel(model=lm, texts=train_texts, dictionary=dictionary, coherence='c_v') # Calculate the coherence score for the topics
        print("Saving model...")
        lm.save(path2model+str(num_topics)) # Save the model
        lm.clear() # Clear the data from the model
        del lm # Delete the model
        gc.collect() # Clears data from the workspace to free up memory
        c_v.append(cm.get_coherence()) # Append the coherence score to the list


    # Save the coherence scores to the file:    
    with open(path2coherence, 'a') as csvFile:
        writer = csv.writer(csvFile)
        writer.writerow(["n_topics","coherence_score"])
        i=2
        for score in c_v:
            print(i)
            writer.writerow([i,score])
            i=i+1

    #Get the best topic model and construct the visualisation

    n=c_v.index(max(c_v))+2 # Get the number of topics with the highest coherence score
    lm = LdaModel.load(path2model+str(n)) # Load the number of topics with the highest coherence score into the workspace
    tm = pyLDAvis.gensim.prepare(lm, mm, dictionary) # Prepare the visualisation
    pyLDAvis.save_html(tm, path2html+str(n)+'.html') # Save the visualisation


    ### Assign topics to texts, using their Codes, along with Probability score:
    # Reorder topic: 

    # Change the topics order to be consistent with the to be consistent with the pyLDAvis topic model (ordered from the most
    # frequent one to the least frequent one) and assign dominant topic to each document:

    # Get the topic order
    to=tm.topic_order

    # set up writing to a file
    # List to hold the processed data for each document
    documents_data = []

    # Loop over all documents in the corpus, and assign topic and probabilities to each
    for i in range(len(corpus)):
        # Get topic probabilities for the document
        topics = lm.get_document_topics(corpus[i])
        # Reorder topics according to pyLDAvis numbering and convert topic probabilities to float
        topics = [["Corpus_Topic " + str(to.index(topic[0] + 1)), float(topic[1])] for topic in topics]
        topics = sorted(topics, key=lambda x: x[0])
        topics_dict = dict(topics)

        # Get dominant topic and its value for the documents
        topics_dict['Corpus_dominant_topic'] = max(topics_dict, key=topics_dict.get)
        topics_dict['Corpus_dominant_value'] = float(topics_dict[topics_dict['Corpus_dominant_topic']])
        topics_dict["Code"] = codes[i]
        topics_dict["text"] = texts[i]

        # Add the document's data to the list
        documents_data.append(topics_dict)

    # Write the data to a JSON file
    with open('TwitterTextTopics.json', 'w', encoding='utf-8') as f:
        json.dump(documents_data, f, ensure_ascii=False, indent=4)
        
    # Categorize Template topics into A and B forms
    Twitter_topics_a = {doc['Code']: doc for doc in documents_data if doc['Code'].startswith('TW.') and doc['Code'].count('.') == 1}


    # Merge function
    def merge_Twitter_data(Twitter_data, Twitter_topics_a):
        for entry in Twitter_data:
            # Merge with form A topics
            code_a = entry.get('Code', '')
            if code_a in Twitter_topics_a:
                entry.update(Twitter_topics_a[code_a])

    # Specify the mongo database and collection
    collection = db['NoRTsDay2Tweets_28.09.23']

    # Load the documents from the collection into a Python list
    Twitter_data = list(collection.find())
    
    # Assuming Twitter_data is your data loaded from MongoDB
    convert_id(Twitter_data) 
    
    # Save the merged data to a JSON file
    with open('Merged_Twitter_Data.json', 'w') as file:
         json.dump(Twitter_data, file, indent=4)

    # Name of the new MongoDB collection
    new_collection_name = "LDA.NoRTsDay2Tweets_28.09.23"

    # Create a new collection (if it doesn't exist)
    new_collection = db[new_collection_name]

    # Insert the merged data
    new_collection.insert_many(Twitter_data)
